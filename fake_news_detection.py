# -*- coding: utf-8 -*-
"""Fake news Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bh3IG5YvMPUmeQXqhDTXl8bivN8WD8c1
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import re
import string

# Step 1: Load the data (CSV file paths should be replaced with actual paths)
true_news_df = pd.read_csv('/content/True (1).csv')
fake_news_df = pd.read_csv('/content/Fake (1).csv')

# Step 2: Preprocess the data
def clean_text(text):
    # Remove punctuation, lowercase text, and remove numbers
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text.lower())
    text = re.sub(r'\d+', '', text)
    return text

# Apply text cleaning to both datasets
true_news_df['text'] = true_news_df['text'].apply(clean_text)
fake_news_df['text'] = fake_news_df['text'].apply(clean_text)

# Add a target column to distinguish between real and fake news
true_news_df['label'] = 1  # Real news
fake_news_df['label'] = 0  # Fake news

# Combine both datasets
data = pd.concat([true_news_df, fake_news_df], ignore_index=True)

# Step 3: Split the data into training and testing sets
X = data['text']
y = data['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Convert text data to numerical data using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 5: Train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train_tfidf, y_train)

# Step 6: Evaluate the model
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f'Model Accuracy: {accuracy * 100:.2f}%')

# Step 7: User input for prediction
def predict_news(news):
    news_cleaned = clean_text(news)
    news_tfidf = vectorizer.transform([news_cleaned])
    prediction = model.predict(news_tfidf)[0]

    if prediction == 1:
        print("The news is REAL.")
    else:
        print("The news is FAKE.")

# Example of user input for prediction
user_news = input("Enter the news text: ")
predict_news(user_news)

#Fake news detection using Logistic Regression model ends here.




#detections using Logistic regression, Bayesian logistic regression and ridge regression. 
# Step 1: Text Preprocessing (Bag of Words Model)
def build_vocabulary(data):
    vocab = {}
    for doc in data:
        for word in doc.split():
            if word not in vocab:
                vocab[word] = len(vocab)
    return vocab

def vectorize(data, vocab):
    vectors = [[0] * len(vocab) for _ in range(len(data))]
    for i, doc in enumerate(data):
        for word in doc.split():
            if word in vocab:
                vectors[i][vocab[word]] += 1
    return vectors

# Step 2: Sigmoid Function
def sigmoid(z):
    return 1 / (1 + (2.718281828459045 ** -z))  # Approximate e

# Step 3A: Logistic Regression Model (Training)
def train_logistic_regression(X, y, learning_rate, num_epochs):
    n_samples = len(X)
    n_features = len(X[0])
    weights = [0] * n_features
    bias = 0

    for epoch in range(num_epochs):
        for i in range(n_samples):
            model = sum(X[i][j] * weights[j] for j in range(n_features)) + bias
            pred = sigmoid(model)
            error = pred - y[i]

            # Gradient Descent update
            for j in range(n_features):
                weights[j] -= learning_rate * error * X[i][j]
            bias -= learning_rate * error

    return weights, bias

# Step 3B: Bayesian Logistic Regression (Training)
def train_bayesian_logistic_regression(X, y, learning_rate, num_epochs, prior_variance=1.0):
    n_samples = len(X)
    n_features = len(X[0])
    weights = [0] * n_features
    bias = 0

    for epoch in range(num_epochs):
        for i in range(n_samples):
            model = sum(X[i][j] * weights[j] for j in range(n_features)) + bias
            pred = sigmoid(model)
            error = pred - y[i]

            # Gradient Descent update with prior
            for j in range(n_features):
                weights[j] -= learning_rate * (error * X[i][j] + (1/prior_variance) * weights[j])
            bias -= learning_rate * error

    return weights, bias

# Step 3C: Ridge Regression (L2 Regularization)
def train_ridge_regression(X, y, learning_rate, num_epochs, l2_penalty=1.0):
    n_samples = len(X)
    n_features = len(X[0])
    weights = [0] * n_features
    bias = 0

    for epoch in range(num_epochs):
        for i in range(n_samples):
            model = sum(X[i][j] * weights[j] for j in range(n_features)) + bias
            error = model - y[i]

            # Gradient Descent update with L2 regularization
            for j in range(n_features):
                weights[j] -= learning_rate * (error * X[i][j] + l2_penalty * weights[j])
            bias -= learning_rate * error

    return weights, bias

# Step 4: Prediction
def predict_logistic(X, weights, bias):
    predictions = []
    for x in X:
        model = sum(x[j] * weights[j] for j in range(len(weights))) + bias
        pred = sigmoid(model)
        predictions.append(1 if pred >= 0.5 else 0)
    return predictions

def predict_ridge(X, weights, bias):
    predictions = []
    for x in X:
        model = sum(x[j] * weights[j] for j in range(len(weights))) + bias
        predictions.append(1 if model >= 0.5 else 0)
    return predictions

# Example Usage
if __name__ == "__main__":
    # Example dataset
    real_news = ["This is real news", "News that is true", "Facts are reported"]
    fake_news = ["This is fake news", "Lies and deception", "Not true at all"]

    # Labels: 1 for real, 0 for fake
    data = real_news + fake_news
    labels = [1, 1, 1, 0, 0, 0]

    # Step 1: Build vocabulary and vectorize the dataset
    vocab = build_vocabulary(data)
    X = vectorize(data, vocab)

    # Step 2: Train Logistic Regression, Bayesian Logistic Regression, and Ridge Regression models
    learning_rate = 0.1
    num_epochs = 1000

    weights_lr, bias_lr = train_logistic_regression(X, labels, learning_rate, num_epochs)
    weights_bayes_lr, bias_bayes_lr = train_bayesian_logistic_regression(X, labels, learning_rate, num_epochs)
    weights_ridge, bias_ridge = train_ridge_regression(X, labels, learning_rate, num_epochs)

    # Step 3: Predict using the trained models
    test_news = ["This is true", "False news and lies"]
    X_test = vectorize(test_news, vocab)

    predictions_lr = predict_logistic(X_test, weights_lr, bias_lr)
    predictions_bayes_lr = predict_logistic(X_test, weights_bayes_lr, bias_bayes_lr)
    predictions_ridge = predict_ridge(X_test, weights_ridge, bias_ridge)

   # Print predictions as True/False
print("Logistic Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_lr])
print("Bayesian Logistic Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_bayes_lr])
print("Ridge Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_ridge])





#individually inplimentation of the regression models codes
#logistic regression
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def train_logistic_regression(X, y, learning_rate, num_epochs):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(num_epochs):
        linear_model = np.dot(X, weights) + bias
        y_pred = sigmoid(linear_model)

        # Gradient descent
        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
        db = (1 / n_samples) * np.sum(y_pred - y)

        weights -= learning_rate * dw
        bias -= learning_rate * db

    return weights, bias

def predict_logistic(X, weights, bias):
    linear_model = np.dot(X, weights) + bias
    y_pred = sigmoid(linear_model)
    return [1 if i >= 0.5 else 0 for i in y_pred]

#Bayesian Logistic Regression
def train_bayesian_logistic_regression(X, y, learning_rate, num_epochs, prior_variance=1.0):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(num_epochs):
        linear_model = np.dot(X, weights) + bias
        y_pred = sigmoid(linear_model)

        # Gradient descent with prior
        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + (1 / prior_variance) * weights
        db = (1 / n_samples) * np.sum(y_pred - y)

        weights -= learning_rate * dw
        bias -= learning_rate * db

    return weights, bias

def predict_bayesian(X, weights, bias):
    linear_model = np.dot(X, weights) + bias
    y_pred = sigmoid(linear_model)
    return [1 if i >= 0.5 else 0 for i in y_pred]

#Ridge Regression
def train_ridge_regression(X, y, learning_rate, num_epochs, l2_penalty=1.0):
    n_samples, n_features = X.shape
    weights = np.zeros(n_features)
    bias = 0

    for epoch in range(num_epochs):
        linear_model = np.dot(X, weights) + bias
        error = linear_model - y

        # Gradient descent with L2 regularization
        dw = (1 / n_samples) * np.dot(X.T, error) + l2_penalty * weights
        db = (1 / n_samples) * np.sum(error)

        weights -= learning_rate * dw
        bias -= learning_rate * db

    return weights, bias

def predict_ridge(X, weights, bias):
    linear_model = np.dot(X, weights) + bias
    return [1 if i >= 0.5 else 0 for i in linear_model]

if __name__ == "__main__":
    # Example dataset
    real_news = ["This is real news", "News that is true", "Facts are reported"]
    fake_news = ["This is fake news", "Lies and deception", "Not true at all"]

    # Labels: 1 for real, 0 for fake
    data = real_news + fake_news
    labels = [1, 1, 1, 0, 0, 0]

    # Step 1: Build vocabulary and vectorize the dataset
    vocab = build_vocabulary(data)
    X = vectorize(data, vocab)

    # Convert X to a NumPy array
    X = np.array(X)

    # Convert labels to numpy array for convenience
    y = np.array(labels)

    # Parameters
    learning_rate = 0.1
    num_epochs = 1000

    # Train models
    weights_lr, bias_lr = train_logistic_regression(X, y, learning_rate, num_epochs)
    weights_bayes_lr, bias_bayes_lr = train_bayesian_logistic_regression(X, y, learning_rate, num_epochs)
    weights_ridge, bias_ridge = train_ridge_regression(X, y, learning_rate, num_epochs)

    # Test data
    test_news = ["This is true", "False news and lies"]
    X_test = vectorize(test_news, vocab)

    # Convert X_test to a NumPy array
    X_test = np.array(X_test)

    # Predictions
    predictions_lr = predict_logistic(X_test, weights_lr, bias_lr)
    predictions_bayes_lr = predict_bayesian(X_test, weights_bayes_lr, bias_bayes_lr)
    predictions_ridge = predict_ridge(X_test, weights_ridge, bias_ridge)

    # Print predictions
    print("Logistic Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_lr])
    print("Bayesian Logistic Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_bayes_lr])
    print("Ridge Regression Predictions:", ["True" if pred == 1 else "False" for pred in predictions_ridge])

#all 3 regression models training with outputs




#**final predicition of output HERE**
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import re
import string

# Load the data
true_news_df = pd.read_csv('/content/True (1).csv')
fake_news_df = pd.read_csv('/content/Fake (1).csv')

# Combine and label the datasets
true_news_df['label'] = 1  # Real news
fake_news_df['label'] = 0  # Fake news
data = pd.concat([true_news_df, fake_news_df], ignore_index=True)

# Preprocess the text data (apply only once)
def clean_text(text):
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text.lower())
    text = re.sub(r'\d+', '', text)
    return text

data['text'] = data['text'].apply(clean_text)

# Split the data into features and labels
X = data['text']
y = data['label']

# Use a smaller TF-IDF vocabulary for speed
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)

# Transform text data with TF-IDF
X_tfidf = vectorizer.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Train a Logistic Regression model with optimized settings
model = LogisticRegression(solver='liblinear')
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Model Accuracy: {accuracy * 100:.2f}%')

# Prediction function for user input
def predict_news(news):
    news_cleaned = clean_text(news)
    news_tfidf = vectorizer.transform([news_cleaned])
    prediction = model.predict(news_tfidf)[0]
    print("The news is REAL." if prediction == 1 else "The news is FAKE.")

# Example of user input for prediction
user_news = input("Enter the news text: ")
predict_news(user_news)








#Random Forest Classifier and xgboost models for comparing the first three regressions models.
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
import re
import string

# Step 1: Load the data
true_news_df = pd.read_csv('/content/True (1).csv')
fake_news_df = pd.read_csv('/content/Fake (1).csv')

# Step 2: Preprocess the data
def clean_text(text):
    # Remove punctuation, lowercase text, and remove numbers
    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text.lower())
    text = re.sub(r'\d+', '', text)
    return text

# Apply text cleaning to both datasets
true_news_df['text'] = true_news_df['text'].apply(clean_text)
fake_news_df['text'] = fake_news_df['text'].apply(clean_text)

# Add a target column to distinguish between real and fake news
true_news_df['label'] = 1  # Real news
fake_news_df['label'] = 0  # Fake news

# Combine both datasets
data = pd.concat([true_news_df, fake_news_df], ignore_index=True)

# Step 3: Split the data into training and testing sets
X = data['text']
y = data['label']

# Check for missing values
X = X.fillna('')
y = y.fillna(0)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Convert text data to numerical data using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Step 5: Train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_tfidf, y_train)

# Evaluate Random Forest model
y_pred_rf = rf_model.predict(X_test_tfidf)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Model Accuracy: {accuracy_rf * 100:.2f}%')
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf, target_names=['Fake', 'Real']))

# Step 6: Train an XGBoost Classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_tfidf, y_train)

# Evaluate XGBoost model
y_pred_xgb = xgb_model.predict(X_test_tfidf)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f'XGBoost Model Accuracy: {accuracy_xgb * 100:.2f}%')
print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb, target_names=['Fake', 'Real']))

# Step 7: Prediction function for user input
def predict_news(model, news):
    news_cleaned = clean_text(news)
    news_tfidf = vectorizer.transform([news_cleaned])
    prediction = model.predict(news_tfidf)[0]
    if prediction == 1:
        print("The news is REAL.")
    else:
        print("The news is FAKE.")

# Example usage
user_news = input("Enter the news text: ")
print("\nPrediction with Random Forest Model:")
predict_news(rf_model, user_news)
print("\nPrediction with XGBoost Model:")
predict_news(xgb_model, user_news)